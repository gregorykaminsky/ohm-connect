{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "469b11c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time data '19/24/2019' does not match format '%m/%d/%y'\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data cleanup. Can be done in python or through SQL. This is the python version. \n",
    "'''\n",
    "import datetime\n",
    "import os\n",
    "file_name = 'event_performance.csv'\n",
    "file_output = 'event_performance_temp.csv'\n",
    "date_format = '%m/%d/%y'\n",
    "output_lines = []\n",
    "with open(file_name, 'r') as f_in:\n",
    "    with open(file_output, 'w') as f_out:\n",
    "        header = f_in.readline()\n",
    "        f_out.write(header)\n",
    "        for line in f_in:\n",
    "            line_arr = line.split(',')\n",
    "            line_arr[0] = line_arr[0].replace('\"', '').strip()\n",
    "            if line_arr[1] != '':\n",
    "                try:\n",
    "                    datetime.datetime.strptime(line_arr[1], date_format)\n",
    "                    new_date = line_arr[1].split('/')\n",
    "                    new_date[2] = '20' + new_date[2]\n",
    "                    line_arr[1] = '/'.join(new_date)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    line_arr[1] = ''\n",
    "            line_arr[2] = line_arr[2].replace('\"', '').strip()\n",
    "            line_arr[3] = line_arr[3].replace('\"', '').replace('?', '')\n",
    "            new_line = ','.join(line_arr)\n",
    "            f_out.write(new_line)\n",
    "os.remove(file_name)\n",
    "os.rename(file_output, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b5bae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "'''\n",
    "    Upload data from local\n",
    "    to s3 bucket ohm-connect-gregory\n",
    "'''\n",
    "bucket_name = 'ohm-connect-gregory'\n",
    "s3 = boto3.resource('s3', aws_access_key_id=os.getenv('aws_access_key'),\n",
    "    aws_secret_access_key=os.getenv('aws_secret_key'))\n",
    "\n",
    "file_name = 'event_performance.csv'\n",
    "obj = s3.Object(bucket_name,file_name)\n",
    "obj.upload_file(file_name)\n",
    "\n",
    "file_name = 'users.csv'\n",
    "obj = s3.Object(bucket_name,file_name)\n",
    "obj.upload_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59c277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Set up the python query method\n",
    "'''\n",
    "cluster_client = boto3.client('redshift', aws_access_key_id=os.getenv('aws_access_key'),\n",
    "    aws_secret_access_key=os.getenv('aws_secret_key'), region_name = 'us-west-2')\n",
    "my_cluster = cluster_client.describe_clusters()['Clusters'][0]\n",
    "\n",
    "data_client = boto3.client('redshift-data', aws_access_key_id=os.getenv('aws_access_key'),\n",
    "    aws_secret_access_key=os.getenv('aws_secret_key'), region_name = 'us-west-2')\n",
    "\n",
    "'''\n",
    "    Designed queries to look into Redshift\n",
    "'''\n",
    "def execute_redshift_query(sql):\n",
    "    print(sql)\n",
    "    output = data_client.execute_statement(\n",
    "                    ClusterIdentifier=my_cluster['ClusterIdentifier'],\n",
    "                    Database='ohmconnect',\n",
    "                    DbUser='gregory',\n",
    "                    Sql=sql)\n",
    "    status = 'SUBMITTED'\n",
    "    cnt = 2\n",
    "    time.sleep(min(cnt, 10))\n",
    "    while cnt < 1000:\n",
    "        result = data_client.describe_statement(Id=output['Id'])\n",
    "        status = result['Status']\n",
    "        if status in {'FAILED', 'ABORTED'}:\n",
    "            print(result['Error'])\n",
    "            print(status)\n",
    "            return (output['Id'], status)\n",
    "        elif status in {'FINISHED'}:\n",
    "            print(status)\n",
    "            return (output['Id'], status)\n",
    "        else:\n",
    "            print('Query status: ' + status)\n",
    "            print(f'Will sleep for {min(cnt, 10)}s \\n')\n",
    "            time.sleep(min(cnt, 10))\n",
    "            cnt += 1\n",
    "    print('Query has been running for too long, result will not be checked anymore.')\n",
    "    print('Last status: ' + status)\n",
    "    return (output['Id'], status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a24625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Create schema\n",
    "    \n",
    "'''\n",
    "\n",
    "schema_name = 'ohm_connect'\n",
    "query = f'''CREATE SCHEMA IF NOT EXISTS {schema_name};'''\n",
    "print(execute_redshift_query(query))\n",
    "\n",
    "\n",
    "'''\n",
    "    Create all the tables, sorted and distributed as necessary\n",
    "'''\n",
    "query = f'''\n",
    "drop table if exists {schema_name}.users;\n",
    "CREATE table {schema_name}.users\n",
    "(\n",
    "    userid VARCHAR(36) NOT NULL PRIMARY KEY\n",
    "    , attribute1 BIGINT encode zstd\n",
    "    , attribute2 VARCHAR encode zstd\n",
    ")\n",
    "DISTSTYLE KEY\n",
    "DISTKEY (userid)\n",
    "SORTKEY (userid);\n",
    "\n",
    "\n",
    "drop table if exists {schema_name}.event_performance;\n",
    "CREATE table {schema_name}.event_performance\n",
    "(\n",
    "    userid VARCHAR(36) NOT NULL PRIMARY KEY\n",
    "    , date DATE encode zstd\n",
    "    , hour INT encode zstd\n",
    "    , points BIGINT encode zstd\n",
    ")\n",
    "DISTSTYLE KEY\n",
    "DISTKEY (userid)\n",
    "SORTKEY (userid);\n",
    "'''\n",
    "print(execute_redshift_query(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bf2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Load the data from s3 bucket into Redshift cluster. \n",
    "'''\n",
    "\n",
    "\n",
    "table_name = 'users'\n",
    "file_name = table_name + '.csv'\n",
    "query = f'''\n",
    "    copy {schema_name}.{table_name}\n",
    "    from 's3://{bucket_name}/{file_name}'\n",
    "    iam_role 'arn:aws:iam::648560186937:role/redshift_role'\n",
    "    format as CSV\n",
    "    IGNOREHEADER 1\n",
    "    delimiter ','\n",
    "    '''\n",
    "print(execute_redshift_query(query))\n",
    "\n",
    "table_name = 'event_performance'\n",
    "file_name = table_name + '.csv'\n",
    "query = f'''\n",
    "    copy {schema_name}.{table_name}\n",
    "    from 's3://{bucket_name}/{file_name}'\n",
    "    iam_role 'arn:aws:iam::648560186937:role/redshift_role'\n",
    "    format as CSV\n",
    "    IGNOREHEADER 1\n",
    "    DATEFORMAT AS 'MM/DD/YYYY'\n",
    "    delimiter ','\n",
    "    '''\n",
    "print(execute_redshift_query(query))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
